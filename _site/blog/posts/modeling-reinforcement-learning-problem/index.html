<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.122">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ahad Jawaid">
<meta name="dcterms.date" content="2023-07-01">
<meta name="description" content="My notes on the second chapter of ‘Grokking Deep Reinforcement Learning’ by Miguel Morales. This post covers the components of the environment and how to model it using Markov Decision Processes (MDPs).">

<title>Ahad Jawaid - Modeling the Reinforcement Learning Problem</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<link href="../../../favicon.ico" rel="icon">
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-Q9WDH7ENSQ"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Q9WDH7ENSQ', { 'anonymize_ip': true});
</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Ahad Jawaid - Modeling the Reinforcement Learning Problem">
<meta property="og:description" content="Ahad Jawaid’s Website">
<meta property="og:image" content="https://ahadjawaid.github.io/personal-website/blog/posts/modeling-reinforcement-learning-problem/assets/blackboard.jpg">
<meta property="og:site-name" content="Ahad Jawaid">
<meta name="twitter:title" content="Ahad Jawaid">
<meta name="twitter:description" content="Ahad Jawaid’s Website">
<meta name="twitter:image" content="https://ahadjawaid.github.io/personal-website/blog/posts/modeling-reinforcement-learning-problem/assets/blackboard.jpg">
<meta name="twitter:creator" content="@_ahadj_">
<meta name="twitter:site" content="@_ahadj_">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../assets/chip.png" alt="computer chip icon" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Ahad Jawaid</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html" rel="" target=""><i class="bi bi-house-door" role="img">
</i> 
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../projects.html" rel="" target=""><i class="bi bi-folder" role="img">
</i> 
 <span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../blog/index.html" rel="" target=""><i class="bi bi-newspaper" role="img">
</i> 
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../talks.html" rel="" target=""><i class="bi bi-megaphone" role="img">
</i> 
 <span class="menu-text">Talks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html" rel="" target=""><i class="bi bi-person" role="img">
</i> 
 <span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools tools-wide">
    <a href="https://twitter.com\_ahadj_" rel="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-twitter"></i></a>
    <a href="https://www.linkedin.com/in/ahad-jawaid" rel="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-linkedin"></i></a>
    <div class="dropdown">
      <a href="" title="" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label=""><i class="bi bi-github"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://github.com/ahadjawaid/personal-website">
            Source Code
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://github.com/ahadjawaid/personal-website/issues">
            Report Bug
            </a>
          </li>
      </ul>
    </div>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#components-of-reinforcement-learning" id="toc-components-of-reinforcement-learning" class="nav-link active" data-scroll-target="#components-of-reinforcement-learning">Components of Reinforcement Learning</a>
  <ul class="collapse">
  <li><a href="#the-agent-the-decision-maker" id="toc-the-agent-the-decision-maker" class="nav-link" data-scroll-target="#the-agent-the-decision-maker">The agent: The decision maker</a></li>
  <li><a href="#the-environment-everything-else" id="toc-the-environment-everything-else" class="nav-link" data-scroll-target="#the-environment-everything-else">The environment: Everything else</a></li>
  </ul></li>
  <li><a href="#unraveling-the-environment" id="toc-unraveling-the-environment" class="nav-link" data-scroll-target="#unraveling-the-environment">Unraveling the Environment:</a>
  <ul class="collapse">
  <li><a href="#states-specific-configurations-of-the-environment" id="toc-states-specific-configurations-of-the-environment" class="nav-link" data-scroll-target="#states-specific-configurations-of-the-environment">States: Specific configurations of the environment</a></li>
  <li><a href="#actions-a-mechanism-to-influence-the-environment" id="toc-actions-a-mechanism-to-influence-the-environment" class="nav-link" data-scroll-target="#actions-a-mechanism-to-influence-the-environment">Actions: A mechanism to influence the environment</a></li>
  <li><a href="#transition-function-consequences-of-agent-actions" id="toc-transition-function-consequences-of-agent-actions" class="nav-link" data-scroll-target="#transition-function-consequences-of-agent-actions">Transition Function: Consequences of agent actions</a></li>
  <li><a href="#reward-function" id="toc-reward-function" class="nav-link" data-scroll-target="#reward-function">Reward Function</a></li>
  <li><a href="#common-types-of-environments" id="toc-common-types-of-environments" class="nav-link" data-scroll-target="#common-types-of-environments">Common Types of Environments:</a></li>
  </ul></li>
  <li><a href="#markov-decision-process-mdps-the-engine-of-the-environment" id="toc-markov-decision-process-mdps-the-engine-of-the-environment" class="nav-link" data-scroll-target="#markov-decision-process-mdps-the-engine-of-the-environment">Markov Decision Process (MDPs): The engine of the environment</a>
  <ul class="collapse">
  <li><a href="#markov-property" id="toc-markov-property" class="nav-link" data-scroll-target="#markov-property">Markov Property</a></li>
  <li><a href="#horizon-time-changes-whats-optimal" id="toc-horizon-time-changes-whats-optimal" class="nav-link" data-scroll-target="#horizon-time-changes-whats-optimal">Horizon: Time changes what’s optimal</a></li>
  <li><a href="#discount-the-future-is-uncertain-value-it-less" id="toc-discount-the-future-is-uncertain-value-it-less" class="nav-link" data-scroll-target="#discount-the-future-is-uncertain-value-it-less">Discount: The future is uncertain, value it less</a></li>
  <li><a href="#extensions-to-mdps" id="toc-extensions-to-mdps" class="nav-link" data-scroll-target="#extensions-to-mdps">Extensions to MDPs</a></li>
  <li><a href="#putting-it-all-together" id="toc-putting-it-all-together" class="nav-link" data-scroll-target="#putting-it-all-together">Putting it all together</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Modeling the Reinforcement Learning Problem</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Deep Reinforcement Learning</div>
    <div class="quarto-category">Notes</div>
  </div>
  </div>

<div>
  <div class="description">
    My notes on the second chapter of ‘Grokking Deep Reinforcement Learning’ by Miguel Morales. This post covers the components of the environment and how to model it using Markov Decision Processes (MDPs).
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ahad Jawaid </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 1, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<style>
.responsive-img {
    width: 100%;
}

@media (min-width: 992px) {
    .responsive-img {
        width: 75%;
    }
}
</style>
<p>This post will discuss how to model the reinforcement learning problem using the mathematical framework of a Markov Decision Process (MDP). We’ll cover what an environment is in reinforcement learning, its components, and how to model it through an MDP.</p>
<p>To understand what we are modeling, we must first familiarize ourselves with the components of reinforcement learning:</p>
<section id="components-of-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="components-of-reinforcement-learning">Components of Reinforcement Learning</h2>
<ul>
<li><em>Reinforcement Learning</em> is A solution for managing complex, uncertain sequential decision-making.
<ul>
<li><em>Complex:</em> Pertains to the large scale of the state and action spaces that the agent needs to navigate.</li>
<li><em>Sequential:</em> Denotes the delayed effect of an agent’s actions, essentially the credit assignment problem.</li>
<li><em>Uncertainty:</em> Highlights the unpredictability of the impact of an agent’s actions on the environment, tying to balance the exploration vs exploitation trade-off.</li>
</ul></li>
</ul>
<p><img src="assets/reinforcement-learning-interaction-cycle.jpg" class="responsive-img"></p>
<section id="the-agent-the-decision-maker" class="level3">
<h3 class="anchored" data-anchor-id="the-agent-the-decision-maker">The agent: The decision maker</h3>
<ul>
<li><strong>Agents:</strong> These are entities solely responsible for making decisions that may influence the environment.</li>
</ul>
<p><img src="assets/internal-steps.jpg" class="responsive-img"></p>
<ul>
<li><strong>Agent’s Improvement Process:</strong>
<ol type="1">
<li>Interact</li>
<li>Evaluate</li>
<li>Improve</li>
</ol></li>
</ul>
</section>
<section id="the-environment-everything-else" class="level3">
<h3 class="anchored" data-anchor-id="the-environment-everything-else">The environment: Everything else</h3>
<ul>
<li>The <em>environment</em> symbolizes the problem at hand. It encapsulates everything external to the agent, anything that’s beyond the agent’s control but responds to the agent’s decisions.</li>
</ul>
<p><img src="assets/environment-process.jpg" class="responsive-img"></p>
<p>Pay attention to how we try to emulate the possible reactions of the environment to the agent’s actions.</p>
</section>
</section>
<section id="unraveling-the-environment" class="level2">
<h2 class="anchored" data-anchor-id="unraveling-the-environment">Unraveling the Environment:</h2>
<ul>
<li>The environment is made up of states, actions, transition probabilities, and a reward function.</li>
</ul>
<section id="states-specific-configurations-of-the-environment" class="level3">
<h3 class="anchored" data-anchor-id="states-specific-configurations-of-the-environment">States: Specific configurations of the environment</h3>
<ul>
<li><p>A <em>state</em> provides a comprehensive description of the environment.</p></li>
<li><p><strong>State Space:</strong> A combiniation of all possible variables and values that can characterize the environment.</p></li>
</ul>
<p><img src="assets/state-space.jpg" class="responsive-img"></p>
<ul>
<li><p><strong>Initial States:</strong> A subset of states where the agent starts in the environment.</p></li>
<li><p><strong>Terminal States:</strong> The final state in an episodic task, any transition from this state will leads back to itself.</p></li>
<li><p><strong>Observation:</strong> The set of variables that the agent perceives. It might not be as comprehensive as a state.</p></li>
<li><p><strong>Observation Space:</strong> All possible values of the variables observed by the agent.</p></li>
</ul>
</section>
<section id="actions-a-mechanism-to-influence-the-environment" class="level3">
<h3 class="anchored" data-anchor-id="actions-a-mechanism-to-influence-the-environment">Actions: A mechanism to influence the environment</h3>
<ul>
<li><p>An <em>action</em> signifies a choice that an agent can make to potentially alter the environment. The set of actions available to an agent may vary across states and during the agent’s learning process.</p></li>
<li><p><strong>Action Space:</strong> The set of all actions in all states.</p></li>
</ul>
</section>
<section id="transition-function-consequences-of-agent-actions" class="level3">
<h3 class="anchored" data-anchor-id="transition-function-consequences-of-agent-actions">Transition Function: Consequences of agent actions</h3>
<ul>
<li>A <em>transition function</em> determines how the environment changes in response to an action.</li>
<li>Denoted as <span class="math inline">\(T(s, a, s')\)</span>, where <span class="math inline">\(s\)</span> is the current state, <span class="math inline">\(a\)</span> is the action taken, and <span class="math inline">\(s'\)</span> is the resulting state.</li>
</ul>
<p><img src="assets/transition-function.jpg" class="responsive-img"></p>
<ul>
<li><p><strong>Stochastic Transitions:</strong> Transitions that don’t guarantee a single resulting state but could lead to multiple states, each with a different probability.</p></li>
<li><p>In RL, we often assume that transition distributions remain stationary, whether they’re stochastic or deterministic. While this assumption can be relaxed to some degree, for most agents, the transitions must at least appear stationary.</p></li>
</ul>
</section>
<section id="reward-function" class="level3">
<h3 class="anchored" data-anchor-id="reward-function">Reward Function</h3>
<ul>
<li><p>A <em>reward function</em> steers the agent towards its goal. It maps a state or an action to a scalar reward, indicative of the goodness but not necessarily the correctness of the agent’s state or action.</p></li>
<li><p>Reward signals supervise your agent. A denser reward signal leads to quicker learning but imposes more bias on the agent’s task-solving approach. Conversely, a sparser (less frequent) reward signal results in a less biased agent, potentially leading to novel, emergent behavior.</p></li>
<li><p><strong>Return:</strong> The sum of the rewards collected in a single episode.</p></li>
</ul>
<p><img src="assets/reward-function.jpg" class="responsive-img"></p>
</section>
<section id="common-types-of-environments" class="level3">
<h3 class="anchored" data-anchor-id="common-types-of-environments">Common Types of Environments:</h3>
<ul>
<li><strong>Grid-World Environment:</strong> An environment that is a square grid of any size.
<ul>
<li><strong>Walk / Corridor:</strong> A type of grid-world environment with a single row.</li>
</ul></li>
<li><strong>Bandit Environment:</strong> An environment with a single non-terminal state, named ‘bandit’ as an analogy to a slot machine that will empty your pockets the same way bandits do.</li>
</ul>
<section id="ways-to-represent-an-environment" class="level4">
<h4 class="anchored" data-anchor-id="ways-to-represent-an-environment">Ways to represent an environment:</h4>
<p><img src="assets/bandit-walk.jpg" class="responsive-img"></p>
<p><img src="assets/bandit-walk-graph.jpg" class="responsive-img"></p>
<p><strong>Table Form</strong></p>
<p><img src="assets/bandit-walk-table.jpg" class="responsive-img"></p>
</section>
</section>
</section>
<section id="markov-decision-process-mdps-the-engine-of-the-environment" class="level2">
<h2 class="anchored" data-anchor-id="markov-decision-process-mdps-the-engine-of-the-environment">Markov Decision Process (MDPs): The engine of the environment</h2>
<ul>
<li><p><em>Markov Decision Processes (MDPs)</em> is a mathematical framework for modeling complex decision-making problems under uncertainty. It consists of system states, per-state actions, a transition function, a reward signal, a horizon, a discount factor, and an initial state distribution.</p></li>
<li><p>In RL, it’s often assumed that all environments operate based on an underlying MDP, whether this assumption holds true or not.</p></li>
</ul>
<section id="markov-property" class="level3">
<h3 class="anchored" data-anchor-id="markov-property">Markov Property</h3>
<ul>
<li>For a state to have the <em>markov property</em>, it must contain all necessary variables to make it independent of all other states.</li>
<li>More formally, the probability of the next state, given the current state and action, is independent of the history of interactions.</li>
</ul>
<p><img src="assets/markov-property.jpg" class="responsive-img"></p>
<ul>
<li>The markov assumption is useful as it allows us to keep the number of variables in a state to a minimum since the more variables you add, the longer it will take the agent to learn. However, completely adhering to the markov assumption may be impractical or even impossible.</li>
</ul>
</section>
<section id="horizon-time-changes-whats-optimal" class="level3">
<h3 class="anchored" data-anchor-id="horizon-time-changes-whats-optimal">Horizon: Time changes what’s optimal</h3>
<ul>
<li><p>A <em>time step</em> is a global clock synchronizing all parties and discretizing time.</p></li>
<li><p>An <em>episode</em> is a sequence of consecutive time steps from the start to the end of an episodic task.</p></li>
<li><p><strong>Planning Horizon:</strong> It’s the temporal depth the agent must plan for.</p></li>
<li><p><strong>Finite Horizon:</strong> It’s a planning horizon that terminates after a specific number of steps.</p></li>
<li><p><strong>Infite / Indefinite Horizon:</strong> It’s an unlimited planning horizon, assuming that the task can continue indefinitely.</p></li>
<li><p><strong>Greedy Horizon:</strong> It’s a planning horizon of a single time step.</p></li>
</ul>
</section>
<section id="discount-the-future-is-uncertain-value-it-less" class="level3">
<h3 class="anchored" data-anchor-id="discount-the-future-is-uncertain-value-it-less">Discount: The future is uncertain, value it less</h3>
<ul>
<li>To handle the possibility of infinite sequences of interactions, we must convey to the agent that immediate rewards are more valuable than future ones. This concept is handled by introducing a <em>discount factor</em> that reduces the value of future rewards, stabilizing the learning of the task.</li>
<li>Moreover, the more we look into the future, the more uncertainty we accumulate. Introducing a discount factor discounts these highly uncertain future rewards, preventing them from affecting our value estimates significantly.</li>
</ul>
<p><img src="assets/effect-of-discount-factor.jpg" class="responsive-img"></p>
<ul>
<li>The discount factor, <span class="math inline">\(\gamma\)</span> or gamma, reduces the variance of the value prediction by considering future, more uncertain, rewards less than immediate ones, fostering urgency in the agent.</li>
</ul>
<p><img src="assets/discount-factor.jpg" class="responsive-img"></p>
</section>
<section id="extensions-to-mdps" class="level3">
<h3 class="anchored" data-anchor-id="extensions-to-mdps">Extensions to MDPs</h3>
<ul>
<li><strong>Partially observable Markov decision process (POMDP):</strong> When the agent can’t fully observe the environment state.</li>
<li><strong>Factored Markov decision process (FMDP):</strong> It compactly represents the transition and reward functions, enabling the representation of large MDPs.</li>
<li><strong>Continuous [Time|Action|State] Markov decision process:</strong> When either time, action, state, or any combination of them are continuous.</li>
<li><strong>Relational Markov decision process (RMDP):</strong> It allows combining probabilistic and relational knowledge.</li>
<li><strong>Semi-Markov decision process (SMDP):</strong> It allows the inclusion of abstract actions that take multiple time steps to complete.</li>
<li><strong>Multi-agent Markov decision process (MMDP):</strong> It allows multiple agents in the same environment.</li>
<li><strong>Decentralized Markov decision process (Dec-MDP):</strong> It allows multiple agents to collaborate and maximize a common reward.</li>
</ul>
</section>
<section id="putting-it-all-together" class="level3">
<h3 class="anchored" data-anchor-id="putting-it-all-together">Putting it all together</h3>
<p><img src="assets/mdps-vs-pomdps.jpg" class="responsive-img"></p>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>Morales, M. (2020). Grokking Deep Reinforcement Learning. Originally Published: October 15, 2020.</p>
<p><em>All figures are sourced from this book.</em></p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    if (id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        for (let i = 0; i < 2; i++) {
          container.appendChild(note.children[i].cloneNode(true));
        }
        return container.innerHTML
      } else {
        return note.innerHTML;
      }
    } else {
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      try { hash = new URL(url).hash; } catch {}
      const id = hash.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note !== null) {
        try {
          const html = processXRef(id, note);
          instance.setContent(html);
        } finally {
          instance.enable();
          instance.show();
        }
      } else {
        // See if we can fetch this
        fetch(url.split('#')[0])
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.getElementById(id);
          console.log(htmlDoc.body.innerHTML);
          if (note !== null) {
            const html = processXRef(id, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="ahadjawaid/ahadjawaid.github.io" data-repo-id="R_kgDOJrWaMQ" data-category="Announcements" data-category-id="DIC_kwDOJrWaMc4CXAIC" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" data-loading="lazy" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="light">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2023, Ahad Jawaid</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com\_ahadj_">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/ahad-jawaid">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/ahadjawaid/ahadjawaid.github.io">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>



<script src="../../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>