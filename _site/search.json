[
  {
    "objectID": "talks/deep-learning-with-fastai/index.html",
    "href": "talks/deep-learning-with-fastai/index.html",
    "title": "Deep learning with fast.ai",
    "section": "",
    "text": "Workshop on deep learning and introducing transfer learning. This work shop was hosted by me and Ibad to teach students how to use the library fast.ai to perform transfer learning on a image classification task.\nVideo Link\nGithub Repository Link\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "üëã Hey, I‚Äôm Ahad",
    "section": "",
    "text": "I‚Äôm a Software Engineer, Researcher, and a Student. This is where I share my experiences, lessons, and projects I work on. I usually work on projects related to artificial intelligence and software engineering. Currently, I am a Software Engineer Intern at Amazon and previously I did research at my university.\nYou can read more about me here.\n \n  \n   \n  \n    \n     Twitter\n  \n  \n    \n     Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "üëã Hey, I‚Äôm Ahad",
    "section": "Recent Posts",
    "text": "Recent Posts\nClick here to check out my blog.\n\n\n\n\n  \n\n\n\n\nIntroduction to Deep Reinforcement Learning\n\n\n\n\n\nMy notes on Deep Reinforcement Learning (DRL) based on the first chapter of the ‚ÄòGrokking Deep Reinforcement Learning‚Äô by Miguel Morales.\n\n\n\n\n\n\nJun 27, 2023\n\n\nAhad Jawaid\n\n\n\n\n\n\n  \n\n\n\n\nFrom Paper to Code: A Guide to Implement Research\n\n\n\n\n\nThis guide covers effective reading, model implementation, code validation, and the power of repetition.\n\n\n\n\n\n\nJun 5, 2023\n\n\nAhad Jawaid\n\n\n\n\n\n\n  \n\n\n\n\nNeural network: The universal function\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\nAhad Jawaid\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/the-universal-function/index.html",
    "href": "blog/posts/the-universal-function/index.html",
    "title": "Neural network: The universal function",
    "section": "",
    "text": "Have you ever imagined a single machine that can adapt to any role given to it? It could be a screw, a tire, a screen or any other task you could think of. That‚Äôs what a neural network is for software ‚Äî a function that can perform any task.\nInitially, when I learned about neural networks, it seemed like a highly abstract concept that emulated the brain. However, after reading books such as Deep Learning by Ian Goodfellow and The Deep Learning practical coder by Jeremy Howard, I came to realize that a neural network is nothing more than a function that can be adjusted to perform any task we require of it."
  },
  {
    "objectID": "blog/posts/the-universal-function/index.html#what-exactly-is-a-function",
    "href": "blog/posts/the-universal-function/index.html#what-exactly-is-a-function",
    "title": "Neural network: The universal function",
    "section": "What exactly is a function?",
    "text": "What exactly is a function?\nA function is a mapping of an input to an output. For instance, an addition function could take two numbers as inputs and output their sum."
  },
  {
    "objectID": "blog/posts/the-universal-function/index.html#adjustable-function",
    "href": "blog/posts/the-universal-function/index.html#adjustable-function",
    "title": "Neural network: The universal function",
    "section": "Adjustable function",
    "text": "Adjustable function\nAn adjustable function, on the other hand, is a function that has weights or parameters that regulate how the inputs are modified to generate a specific output. For example, a function f that takes two numbers as inputs can be expressed as\n\nf(input1, input2) = weight1 * input1 + weight2 * input2\n\nHere, we can change the function by adjusting the weights."
  },
  {
    "objectID": "blog/posts/the-universal-function/index.html#how-can-an-adjustable-function-learn-a-task",
    "href": "blog/posts/the-universal-function/index.html#how-can-an-adjustable-function-learn-a-task",
    "title": "Neural network: The universal function",
    "section": "How can an adjustable function learn a task?",
    "text": "How can an adjustable function learn a task?\nNow that we have established what an adjustable function is, let‚Äôs look at how we can use it to solve a problem. The process of adjusting the weights in the function to accomplish a task involves three main steps:\n1. Get output from the function\n2. Check how wrong the output is compared to 3. the desired output\n4. Adjust the weights to make the output look more like the desired output\nThis process can be illustrated in the following diagram:\n\nAs simple as this may seem, this is the core of how a neural network learns a task. It starts by guessing an output, and then we adjust the weights to make it look more like the desired output."
  },
  {
    "objectID": "blog/posts/the-universal-function/index.html#adjusting-the-weights-automatically",
    "href": "blog/posts/the-universal-function/index.html#adjusting-the-weights-automatically",
    "title": "Neural network: The universal function",
    "section": "Adjusting the weights automatically",
    "text": "Adjusting the weights automatically\nAdjusting the weights automatically is where it gets interesting. Let‚Äôs assume you‚Äôre playing a game where someone has a number in mind, and you have to guess it. Every time you make a guess, they tell you whether you‚Äôre close or far. If you randomly pick the numbers at random, you‚Äôd get nowhere. But adding two simple steps to the process can solve this problem:\n1. A method to determine how far you are from the desired output (output error method)\n2.A method for finding the direction and extent of the weights to be changed (method for determining how to adjust weights)"
  },
  {
    "objectID": "blog/posts/the-universal-function/index.html#output-error-method",
    "href": "blog/posts/the-universal-function/index.html#output-error-method",
    "title": "Neural network: The universal function",
    "section": "Output Error Method",
    "text": "Output Error Method\nTo find how far we are from the desired output, we can subtract our function‚Äôs output and the desired output,\n\nf(input1, input2) = 5 goal_function(input1, input2) = 10 error = 5‚Äì10 = -5\n\nThe problem with this method is that it could give us negative values, which can be problematic when attempting to minimize the error. To address this issue, we can bound the method by taking the absolute value of the subtraction (positive number).\n\nerror = | 5‚Äì10 | = 5"
  },
  {
    "objectID": "blog/posts/the-universal-function/index.html#method-for-determining-how-to-adjust-weights",
    "href": "blog/posts/the-universal-function/index.html#method-for-determining-how-to-adjust-weights",
    "title": "Neural network: The universal function",
    "section": "Method for determining how to adjust weights",
    "text": "Method for determining how to adjust weights\nThe last addition to adjust the weights automatically is to find the direction and extent of the weights to be changed. We can accomplish this by using a concept from calculus known as the derivative, which is the slope of the function at a specific point. By finding the slope or derivative, we can determine the direction and amount to change the weights.\n\nSo in this illustration, we can find that the slope at point one is two by using the rise-over run of the tangent line (a line that touches the function at a point). We can use this slope or derivative to determine in what direction and how much to change the weights. This is done because of a property that a derivative has, which is when the derivative is zero, it is at the minimum or maximum (or saddle point) of the function. We can take advantage of this property with our method of measuring the error to find the minimum of the error.\nSo if we keep adjusting the weights and the derivative gets smaller, we are approaching a minimum or maximum (or saddle point). To ensure we are finding the minimum and not the maximum error, we need to figure out what direction we should change the weights. This can be done simply by subtracting the derivative to act as we descend to the minimum. Putting it together\nThe heart of deep learning lies in the ability of neural networks to learn any task by going through five steps. These are:\n1. Input data into the function\n2. Compare the output to the error function (loss function)\n3. Take the derivative of the error function with respect to the weights\n4. Subtract the derivative of the weights from the weights\n5. Repeat until the error is small\n\nEverything else is focused on making the training process efficient and timely."
  },
  {
    "objectID": "blog/posts/the-universal-function/index.html#the-universal-function",
    "href": "blog/posts/the-universal-function/index.html#the-universal-function",
    "title": "Neural network: The universal function",
    "section": "The Universal Function",
    "text": "The Universal Function\nIn conclusion, the neural network is a powerful tool that allows software to adapt to any role given to them. At first glance, it may seem like an abstract concept, but it is nothing more guessing and checking then improving. What‚Äôs fascinating about this is that the improvement can be made automatically by using concepts from calculus like the derivative, which allows us to determine how to improve. With these tools, the neural network can learn any task and solve most problem thrown its way.\nToday, we see the applications of neural networks in various fields, including speech recognition, image and pattern recognition, and natural language processing, to name a few. The possibilities of what we can achieve with this technology are endless, and we are only scratching the surface of what it can do."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nIntroduction to Deep Reinforcement Learning\n\n\n\n\n\nMy notes on Deep Reinforcement Learning (DRL) based on the first chapter of the ‚ÄòGrokking Deep Reinforcement Learning‚Äô by Miguel Morales.\n\n\n\n\n\n\n27 June 2023\n\n\nAhad Jawaid\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nFrom Paper to Code: A Guide to Implement Research\n\n\n\n\n\nThis guide covers effective reading, model implementation, code validation, and the power of repetition.\n\n\n\n\n\n\n05 June 2023\n\n\nAhad Jawaid\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nNeural network: The universal function\n\n\n\n\n\n\n\n\n\n\n\n\n23 February 2023\n\n\nAhad Jawaid\n\n\n6 min\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Ahad Jawaid",
    "section": "",
    "text": "Twitter\n  \n  \n    \n     Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n\n      \nI‚Äôm a curious individual who is broadly interested in topics related to creating intelligent agents.\nCurrently I‚Äôm a Software Development Engineer at Amazon, working on the spoken understanding team at Alexa smart home. Before that I was an Undergraduate Researcher at labs at my university. I‚Äôm currently a senior computer science major at The University of Texas at Dallas.\nI enjoy reading ML papers, reading books, playing musical instruments, and running.\n\n    \n    \n  \n\n\n\nExperienceEducationOrganizationsCertifications\n\n\n\n\nSoftware Developer Engineer Intern\nAmazon, Seattle, WA, USA\n\n\nMay 2023 - Present\n\n\n\n\nIndependent Undergraduate Researcher\nThe University of Texas at Dallas, Richardson, TX, USA\n\n\nJanuary 2023 - May 2023\n\n\n\n\nUndergraduate Researcher\nSystem Security Research Lab, Richardson, TX, USA\n\n\nMay 2022 - Septemeber 2022\n\n\n\n\n\n\nBachelors of Science in Computer Science\nThe University of Texas at Dallas, Richardson, TX, USA\n\n\nExpected Graduation: May 2024\n\n\n\n\n\n\nArtificial Intelligence Society\nDevelopment Officer\n\n\nOctober 2022 ‚Äì Present\n\n\n\n\nAssociation of Computing Machinery\nCommunity Member, Mentor, Mentee, TIP Participant\n\n\nJanuary 2022 ‚Äì Present\n\n\n\n\nFinTech UTD\nDeveloper\n\n\nSeptember 2022 ‚Äì Novemeber 2022\n\n\n\n\nComet Marketing\nDirector of Web Development\n\n\nAugust 2022 ‚Äì October 2022\n\n\n\n\nUX Club\nMember\n\n\nJanuary 2022 ‚Äì May 2022\n\n\n\n\n\n\nNeural Networks and Deep Learning, Deeplearning.ai, Coursera\nCertificate\n\n\nJanuary, 2023\n\n\n\n\nStructuring Machine Learning Projects Structuring, Deeplearning.ai, Coursera\nCertificate\n\n\nJanuary, 2023\n\n\n\n\nCertified Developer - Associate, AWS\nCredential ID NGJGEZLJFJE4QMWQ\n\n\nMay, 2022"
  },
  {
    "objectID": "blog/posts/introduction-to-deep-reinforcement-learning/index.html",
    "href": "blog/posts/introduction-to-deep-reinforcement-learning/index.html",
    "title": "Introduction to Deep Reinforcement Learning",
    "section": "",
    "text": "Artificial Intelligence (AI) is a domain of computer science dedicated to developing software capable of exhibiting attributes of intelligence."
  },
  {
    "objectID": "blog/posts/introduction-to-deep-reinforcement-learning/index.html#artificial-intelligence-ai",
    "href": "blog/posts/introduction-to-deep-reinforcement-learning/index.html#artificial-intelligence-ai",
    "title": "Introduction to Deep Reinforcement Learning",
    "section": "",
    "text": "Artificial Intelligence (AI) is a domain of computer science dedicated to developing software capable of exhibiting attributes of intelligence."
  },
  {
    "objectID": "blog/posts/introduction-to-deep-reinforcement-learning/index.html#machine-learning-ml",
    "href": "blog/posts/introduction-to-deep-reinforcement-learning/index.html#machine-learning-ml",
    "title": "Introduction to Deep Reinforcement Learning",
    "section": "Machine Learning (ML)",
    "text": "Machine Learning (ML)\n\nMachine learning, a subset of AI, tackles problems necessitating intelligent solutions by learning from data.\nSupervised Learning (SL): A method that learns from labeled data.\n\nE.g., handwritten-digit-recognition\n\nUnsupervised Learning (UL): A method that learns from unlabeled data\n\nE.g., customer segmentaiton\n\nReinforcement Learning (RL): A method that learns from trial and error\n\nE.g., pong-playing agent"
  },
  {
    "objectID": "blog/posts/introduction-to-deep-reinforcement-learning/index.html#deep-learning-dl",
    "href": "blog/posts/introduction-to-deep-reinforcement-learning/index.html#deep-learning-dl",
    "title": "Introduction to Deep Reinforcement Learning",
    "section": "Deep Learning (DL)",
    "text": "Deep Learning (DL)\n\nDeep Learning employs multi-layered non-linear function approximations, also known as neural networks, to address ML tasks. Essentially, it is a suite of techniques that utilize neural networks to solve ML challenges.\n\n\n\nDeep Reinforcement Learning (DRL)\n\nDeep Reinforcement Learning learns through trial and error from feedback that‚Äôs simultaneously sequentially, evaluative, and sampled by leveraging non-linear function approximation (neural networks)."
  },
  {
    "objectID": "blog/posts/introduction-to-deep-reinforcement-learning/index.html#reinforcement-learning-rl",
    "href": "blog/posts/introduction-to-deep-reinforcement-learning/index.html#reinforcement-learning-rl",
    "title": "Introduction to Deep Reinforcement Learning",
    "section": "Reinforcement Learning (RL)",
    "text": "Reinforcement Learning (RL)\n\nSimilar fields\n\nReinforcement Learning (RL): Investigates methods of resolving intricate sequential decision-making problems under uncertain conditions.\nControl Theory (CT): Examines methods of controlling complex known dynamic systems.\nOperations Research (OR): Investigates decision-making under uncertain conditions, generally featuring a larger action space than in DRL.\nPsychology: Studies human behavior, which frequently encapsulates complex sequential decision-making problems under uncertainty.\n\n\n\n\nAgent and Enviroment\n\nAgent: Refers exclusively to the decision-making entity.\n\nIf you are training a robot arm to pick up a toy. Only the code that makes decision is the agent not the robot arm\n\nEnvironment: Includes everything external to the agent, beyond the agent‚Äôs control, and everything that follows the agent‚Äôs decisions.\n\nIf you are training a robot arm to pick up a toy. objects to be picked up, the tray where the objects lay, the wind, and the arm are parts of the environment\n\n\n\n\n\nStates and Observations\n\nState Space: The set of all possible variables and values that can represent the state of the environment.\nState: A comprehensive description of the environment, or an instantiation of the state space.\nObservation: A partial or incomplete description of the environment.\n\n\n\n\nReinforcement Leraning Cycle\n\nTransition Function: The mapping from the agent‚Äôs action to a potentially new state.\nReward Function: The mapping from the action taken to the potential reward signal.\n\nGoals are defined via the reward function.\n\nModel: A set of the transitions and rewards.\n\n\n\nAgent‚Äôs Improvement process:\n\nInteract with the environment.\nEvaluates its behavior.\nImproves its responses.\n\n\n\nAgent‚Äôs can be designed to learn:\n\nPolicy: The mapping from observations to actions.\nModels: A model of the environment on mappings.\nValue Functions: The mapping of a state to its estimated value.\n\n\n\n\nExperiences\n\nTime Step: A single cycle of interaction between the agent and the environment.\nExperience: The set consisting of the state, the action, the reward, and the new state in a single time step.\n\n\n\nEpisodic Task: Tasks that have a natural ending or goes on finitely many step.\n\nE.g., video games\n\nContinuing Task: Tasks that don‚Äôt have a natural ending or could go on indefinitely.\n\nE.g., learning forward motion\n\n\n\n\nCredit Assignment Problem\n\nTemporal Credit Assignment Problem: the challenge in determining which state and/or action is responsible for a reward the agent recieves\n\nUsually occurs when the agent may have delayed rewards from an action or state that caused it hence the temporal aspect of the problem\n\n\n\n\n\nExploration vs.¬†Exploitation\n\nEvaluative Feedback: Feedback that provides an indication of performance but not correctness.\nExploration versus Exploitation trade-off: The balance between collecting new information from the environment and using known information to maximize rewards.\n\n\n\n\nSampled Feedback\n\nLearning from sparse or weak feedback becomes more challenging with samples only. The agent must be capable of generalizing to learn from sampled feedback.\n\n\n\n\nTypes of Agents\n\nPolicy-based: Designed to approximate policies.\nModel-based: Designed to approximate models.\nValue-based: Designed to approximate value functions.\nActor-critic: Designed to approximate both policies and value functions.\n\n\n\nPros and Cons\nStrength: Reinforcement learning excels in mastering specific tasks.\nWeaknesses: To learn a well-performing policy, it generally requires millions of samples."
  },
  {
    "objectID": "blog/posts/introduction-to-deep-reinforcement-learning/index.html#history-of-deep-reinforcement-learning",
    "href": "blog/posts/introduction-to-deep-reinforcement-learning/index.html#history-of-deep-reinforcement-learning",
    "title": "Introduction to Deep Reinforcement Learning",
    "section": "History of Deep Reinforcement Learning",
    "text": "History of Deep Reinforcement Learning\n\nAlan Turing - 1930: Developed the Turing Test, a test of a machine‚Äôs ability to exhibit intelligent behavior indistinguishable from that of a human.\nJohn McCarthy - 1955: Coined the term ‚ÄúArtificial Intelligence‚Äù.\nAndrew Ng - 2002: Trained an autonomous helicopter to perform stunts by observing human-expert flights using inverse reinforcement learning.\nNate Kohl and Peter Stone - 2002: Applied policy-gradient methods to train a soccer-playing robot.\nMnih et al.¬†- 2013, 2015: Introduced the DQN algorithm, which learned to play Atari games from raw pixels.\n\n\n\nSilver et al.¬†- 2014: Released the deterministic policy gradient (DPG) algorithm.\nLillicrap et al.¬†- 2015: Improved DPG with deep deterministic policy gradient (DDPG)\nSchulman et al.¬†- 2016: Released trust region policy optimization (TRPO) and generalized advantage estimation (GAE) methods.\nSergey Levine et al.¬†- 2016: Published Guided Policy Search (GPS)\nSilver et al.¬†- 2016: Demonstrated AlphaGo\n‚Ä¶"
  },
  {
    "objectID": "blog/posts/introduction-to-deep-reinforcement-learning/index.html#references",
    "href": "blog/posts/introduction-to-deep-reinforcement-learning/index.html#references",
    "title": "Introduction to Deep Reinforcement Learning",
    "section": "References",
    "text": "References\nMorales, M. (2020). Grokking Deep Reinforcement Learning. Originally Published: October 15, 2020.\nAll figures are sourced from this book."
  },
  {
    "objectID": "blog/posts/translating-theory-into-code/index.html",
    "href": "blog/posts/translating-theory-into-code/index.html",
    "title": "From Paper to Code: A Guide to Implement Research",
    "section": "",
    "text": "Recently, I‚Äôve been diving into the practice of implementing deep learning research papers. This post is designed to guide you through this process, sharing my key insights along the way.\nStarting with an independent study course at university, I explored deep learning intensively. To streamline the journey of understanding and implementing such papers, I‚Äôve distilled the process into four steps:\n\nLet‚Äôs break each one down."
  },
  {
    "objectID": "blog/posts/translating-theory-into-code/index.html#introduction",
    "href": "blog/posts/translating-theory-into-code/index.html#introduction",
    "title": "From Paper to Code: A Guide to Implement Research",
    "section": "",
    "text": "Recently, I‚Äôve been diving into the practice of implementing deep learning research papers. This post is designed to guide you through this process, sharing my key insights along the way.\nStarting with an independent study course at university, I explored deep learning intensively. To streamline the journey of understanding and implementing such papers, I‚Äôve distilled the process into four steps:\n\nLet‚Äôs break each one down."
  },
  {
    "objectID": "blog/posts/translating-theory-into-code/index.html#read",
    "href": "blog/posts/translating-theory-into-code/index.html#read",
    "title": "From Paper to Code: A Guide to Implement Research",
    "section": "Read",
    "text": "Read\nReading a research paper should be focused on understanding the process rather than memorizing details. Pay attention to the inputs, outputs, data transformations, and assumptions.\n\nTo expedite the reading process, it helps to know where to find necessary details. Typically, the most crucial sections in a research paper are:\n\nAbstract\n\nProvides a broad understanding of the paper and its relevance.\n\nFigures\n\nOften encapsulate the paper‚Äôs content and are valuable for understanding the model‚Äôs workings. However, ensure their accuracy by cross-referencing the textual content.\n\nIntroduction\n\nHighlights the paper‚Äôs novelty or significance.\n\nArchitecture (Can go by other names)\n\nContains detailed descriptions of the architecture (usually where most of your time will be spent).\n\nTraining\n\nProvides hyperparameters for training, model performance, and other essential information.\n\nThe appendix\n\nOften contains implementation details not fitting into the main paper‚Äôs flow.\n\nExperiements (Optional)\n\nCan include important data and parameters used for model training."
  },
  {
    "objectID": "blog/posts/translating-theory-into-code/index.html#implement",
    "href": "blog/posts/translating-theory-into-code/index.html#implement",
    "title": "From Paper to Code: A Guide to Implement Research",
    "section": "Implement",
    "text": "Implement\nDuring implementation, the key is not to get overwhelmed by the complexity of the task. Instead, focus on the components you understand, and incrementally build upon them. It‚Äôs okay if your code isn‚Äôt flawless initially; revisions can be made as you progress."
  },
  {
    "objectID": "blog/posts/translating-theory-into-code/index.html#visualize",
    "href": "blog/posts/translating-theory-into-code/index.html#visualize",
    "title": "From Paper to Code: A Guide to Implement Research",
    "section": "Visualize",
    "text": "Visualize\nPost-implementation, validate your code. Use libraries like matplotlib to visualize the outputs, or tools like pytest to perform tests. Working in an interactive environment like a jupyter notebook can also be helpful."
  },
  {
    "objectID": "blog/posts/translating-theory-into-code/index.html#repeat",
    "href": "blog/posts/translating-theory-into-code/index.html#repeat",
    "title": "From Paper to Code: A Guide to Implement Research",
    "section": "Repeat",
    "text": "Repeat\nThe key to successful implementation is repetition. Even if you start slowly, your pace will increase as you build your knowledge and develop better abstractions. This process also improves your programming skills."
  },
  {
    "objectID": "blog/posts/translating-theory-into-code/index.html#conclusion",
    "href": "blog/posts/translating-theory-into-code/index.html#conclusion",
    "title": "From Paper to Code: A Guide to Implement Research",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, the methodology I formulated and the key lessons I learned during my initial experience implementing a research paper can be encapsulated in four words: read, implement, visualize, and repeat. I hope you find this post beneficial, and I welcome any feedback or suggestions you might have. Thank you!"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nDeep learning with fast.ai\n\n\n\n\n\n\n\n\n\n\n\n\n22 March 2023\n\n\nAhad Jawaid\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n Back to top"
  }
]