<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Ahad Jawaid</title>
<link>https://ahadjawaid.github.io/personal-website/blog/index.html</link>
<atom:link href="https://ahadjawaid.github.io/personal-website/blog/index.xml" rel="self" type="application/rss+xml"/>
<description>Ahad Jawaid&#39;s Website!</description>
<generator>quarto-1.4.122</generator>
<lastBuildDate>Tue, 27 Jun 2023 07:00:00 GMT</lastBuildDate>
<item>
  <title>Introduction to Deep Reinforcement Learning</title>
  <dc:creator>Ahad Jawaid</dc:creator>
  <link>https://ahadjawaid.github.io/personal-website/blog/posts/introduction-to-deep-reinforcement-learning/index.html</link>
  <description><![CDATA[ 



<style>
.responsive-img {
    width: 100%;
}

@media (min-width: 992px) {
    .responsive-img {
        width: 75%;
    }
}
</style>
<section id="artificial-intelligence-ai" class="level2">
<h2 class="anchored" data-anchor-id="artificial-intelligence-ai">Artificial Intelligence (AI)</h2>
<ul>
<li>Artificial Intelligence (AI) is a domain of computer science dedicated to developing software capable of exhibiting attributes of intelligence.</li>
</ul>
<p><img src="https://ahadjawaid.github.io/personal-website/blog/posts/introduction-to-deep-reinforcement-learning/assets/subfields_of_artificial_intelligence.jpg" class="responsive-img"></p>
</section>
<section id="machine-learning-ml" class="level2">
<h2 class="anchored" data-anchor-id="machine-learning-ml">Machine Learning (ML)</h2>
<ul>
<li>Machine learning, a subset of AI, tackles problems necessitating intelligent solutions by learning from data.</li>
<li><strong>Supervised Learning (SL):</strong> A method that learns from labeled data.
<ul>
<li>E.g., handwritten-digit-recognition</li>
</ul></li>
<li><strong>Unsupervised Learning (UL):</strong> A method that learns from unlabeled data
<ul>
<li>E.g., customer segmentaiton</li>
</ul></li>
<li><strong>Reinforcement Learning (RL):</strong> A method that learns from trial and error
<ul>
<li>E.g., pong-playing agent</li>
</ul></li>
</ul>
<p><img src="https://ahadjawaid.github.io/personal-website/blog/posts/introduction-to-deep-reinforcement-learning/assets/main_branches_of_machine_learning.jpg" class="responsive-img"></p>
</section>
<section id="deep-learning-dl" class="level2">
<h2 class="anchored" data-anchor-id="deep-learning-dl">Deep Learning (DL)</h2>
<ul>
<li>Deep Learning employs multi-layered non-linear function approximations, also known as neural networks, to address ML tasks. Essentially, it is a suite of techniques that utilize neural networks to solve ML challenges.</li>
</ul>
<p><img src="https://ahadjawaid.github.io/personal-website/blog/posts/introduction-to-deep-reinforcement-learning/assets/deep_learning_is_a_powerful_toolbox.jpg" class="responsive-img"></p>
<section id="deep-reinforcement-learning-drl" class="level3">
<h3 class="anchored" data-anchor-id="deep-reinforcement-learning-drl">Deep Reinforcement Learning (DRL)</h3>
<ul>
<li>Deep Reinforcement Learning learns through trial and error from feedback that’s simultaneously sequentially, evaluative, and sampled by leveraging non-linear function approximation (neural networks).</li>
</ul>
</section>
</section>
<section id="reinforcement-learning-rl" class="level2">
<h2 class="anchored" data-anchor-id="reinforcement-learning-rl">Reinforcement Learning (RL)</h2>
<section id="similar-fields" class="level3">
<h3 class="anchored" data-anchor-id="similar-fields">Similar fields</h3>
<ul>
<li><strong>Reinforcement Learning (RL):</strong> Investigates methods of resolving intricate sequential decision-making problems under uncertain conditions.</li>
<li><strong>Control Theory (CT)</strong>: Examines methods of controlling complex known dynamic systems.</li>
<li><strong>Operations Research (OR)</strong>: Investigates decision-making under uncertain conditions, generally featuring a larger action space than in DRL.</li>
<li><strong>Psychology</strong>: Studies human behavior, which frequently encapsulates complex sequential decision-making problems under uncertainty.</li>
</ul>
<p><img src="https://ahadjawaid.github.io/personal-website/blog/posts/introduction-to-deep-reinforcement-learning/assets/the_synergy_between_similar_fields.jpg" class="responsive-img"></p>
</section>
<section id="agent-and-enviroment" class="level3">
<h3 class="anchored" data-anchor-id="agent-and-enviroment">Agent and Enviroment</h3>
<ul>
<li><strong>Agent:</strong> Refers exclusively to the decision-making entity.
<ul>
<li>For instance, if you are training a robot arm to pick up a toy, the agent is the code that makes the decisions, not the robot arm itself.</li>
</ul></li>
<li><strong>Environment:</strong> Includes everything external to the agent, beyond the agent’s control, and everything that follows the agent’s decisions.
<ul>
<li>In the context of training a robot arm to pick up a toy, the objects to be picked up, the tray where the objects reside, atmospheric conditions like wind, and even the robot arm itself are all considered parts of the environment.</li>
</ul></li>
</ul>
<p><img src="https://ahadjawaid.github.io/personal-website/blog/posts/introduction-to-deep-reinforcement-learning/assets/boundary_between_agent_and_environment.jpg" class="responsive-img"></p>
</section>
<section id="states-and-observations" class="level3">
<h3 class="anchored" data-anchor-id="states-and-observations">States and Observations</h3>
<ul>
<li><strong>State Space:</strong> The set of all possible variables and values that can represent the state of the environment.</li>
<li><strong>State:</strong> A comprehensive description of the environment, or an instantiation of the state space.</li>
<li><strong>Observation:</strong> A partial or incomplete description of the environment.</li>
</ul>
<p><img src="https://ahadjawaid.github.io/personal-website/blog/posts/introduction-to-deep-reinforcement-learning/assets/states_vs_observations.jpg" class="responsive-img"></p>
</section>
<section id="reinforcement-learning-cycle" class="level3">
<h3 class="anchored" data-anchor-id="reinforcement-learning-cycle">Reinforcement Learning Cycle</h3>
<ul>
<li><strong>Transition Function:</strong> The mapping from the agent’s action to a potentially new state.</li>
<li><strong>Reward Function:</strong> The mapping from the action taken to the potential reward signal.
<ul>
<li>Goals are defined via the reward function.</li>
</ul></li>
<li><strong>Model:</strong> A set of the transitions and rewards.</li>
</ul>
<p><img src="https://ahadjawaid.github.io/personal-website/blog/posts/introduction-to-deep-reinforcement-learning/assets/the_reinforcement_learning_cycle.jpg" class="responsive-img"></p>
<section id="agents-improvement-process" class="level4">
<h4 class="anchored" data-anchor-id="agents-improvement-process">Agent’s Improvement process:</h4>
<ol type="1">
<li>Interact with the environment.</li>
<li>Evaluates its behavior.</li>
<li>Improves its responses.</li>
</ol>
</section>
<section id="agents-can-be-designed-to-learn" class="level4">
<h4 class="anchored" data-anchor-id="agents-can-be-designed-to-learn">Agent’s can be designed to learn:</h4>
<ul>
<li><strong>Policy:</strong> The mapping from observations to actions.</li>
<li><strong>Models:</strong> A model of the environment on mappings.</li>
<li><strong>Value Functions:</strong> The mapping of a state to its estimated value.</li>
</ul>
</section>
</section>
<section id="experiences" class="level3">
<h3 class="anchored" data-anchor-id="experiences">Experiences</h3>
<ul>
<li><strong>Time Step:</strong> A single cycle of interaction between the agent and the environment.</li>
<li><strong>Experience:</strong> The set consisting of the state, the action, the reward, and the new state in a single time step.</li>
</ul>
<p><img src="https://ahadjawaid.github.io/personal-website/blog/posts/introduction-to-deep-reinforcement-learning/assets/experience_tuples.jpg" class="responsive-img"></p>
<ul>
<li><strong>Episodic Task:</strong> Tasks that have a natural ending or goes on finitely many step.
<ul>
<li>E.g., video games</li>
</ul></li>
<li><strong>Continuing Task:</strong> Tasks that don’t have a natural ending or could go on indefinitely.
<ul>
<li>E.g., learning forward motion</li>
</ul></li>
</ul>
</section>
<section id="credit-assignment-problem" class="level3">
<h3 class="anchored" data-anchor-id="credit-assignment-problem">Credit Assignment Problem</h3>
<ul>
<li><strong>Temporal Credit Assignment Problem:</strong> the challenge in determining which state and/or action is responsible for a reward the agent recieves
<ul>
<li>Usually occurs when the agent may have delayed rewards from an action or state that caused it hence the temporal aspect of the problem</li>
</ul></li>
</ul>
<p><img src="https://ahadjawaid.github.io/personal-website/blog/posts/introduction-to-deep-reinforcement-learning/assets/temporal_credit_assignment_problem.jpg" class="responsive-img"></p>
</section>
<section id="exploration-vs.-exploitation" class="level3">
<h3 class="anchored" data-anchor-id="exploration-vs.-exploitation">Exploration vs.&nbsp;Exploitation</h3>
<ul>
<li><strong>Evaluative Feedback:</strong> Feedback that provides an indication of performance but not correctness.</li>
<li><strong>Exploration versus Exploitation trade-off:</strong> The balance between collecting new information from the environment and using known information to maximize rewards.</li>
</ul>
<p><img src="https://ahadjawaid.github.io/personal-website/blog/posts/introduction-to-deep-reinforcement-learning/assets/exploration_vs_exploitation.jpg" width="75%"></p>
</section>
<section id="sampled-feedback" class="level3">
<h3 class="anchored" data-anchor-id="sampled-feedback">Sampled Feedback</h3>
<ul>
<li>Learning from sparse or weak feedback becomes more challenging with samples only. The agent must be capable of generalizing to learn from sampled feedback.</li>
</ul>
<p><img src="https://ahadjawaid.github.io/personal-website/blog/posts/introduction-to-deep-reinforcement-learning/assets/learning_from_sampled_feedback.jpg" width="75%"></p>
</section>
<section id="types-of-agents" class="level3">
<h3 class="anchored" data-anchor-id="types-of-agents">Types of Agents</h3>
<ul>
<li><strong>Policy-based:</strong> Designed to approximate policies.</li>
<li><strong>Model-based:</strong> Designed to approximate models.</li>
<li><strong>Value-based:</strong> Designed to approximate value functions.</li>
<li><strong>Actor-critic:</strong> Designed to approximate both policies and value functions.</li>
</ul>
</section>
<section id="pros-and-cons" class="level3">
<h3 class="anchored" data-anchor-id="pros-and-cons">Pros and Cons</h3>
<p><strong>Strength:</strong> Reinforcement learning excels in mastering specific tasks.</p>
<p><strong>Weaknesses:</strong> To learn a well-performing policy, it generally requires millions of samples.</p>
</section>
</section>
<section id="history-of-deep-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="history-of-deep-reinforcement-learning">History of Deep Reinforcement Learning</h2>
<ul>
<li><p><strong>Alan Turing - 1930:</strong> Developed the Turing Test, a test of a machine’s ability to exhibit intelligent behavior indistinguishable from that of a human.</p></li>
<li><p><strong>John McCarthy - 1955:</strong> Coined the term “Artificial Intelligence”.</p></li>
<li><p><strong>Andrew Ng - 2002:</strong> Trained an autonomous helicopter to perform stunts by observing human-expert flights using inverse reinforcement learning.</p></li>
<li><p><strong>Nate Kohl and Peter Stone - 2002:</strong> Applied policy-gradient methods to train a soccer-playing robot.</p></li>
<li><p><strong>Mnih et al.&nbsp;- 2013, 2015:</strong> Introduced the DQN algorithm, which learned to play Atari games from raw pixels.</p>
<ul>
<li><img src="https://ahadjawaid.github.io/personal-website/blog/posts/introduction-to-deep-reinforcement-learning/assets/atari_dqn.jpg" width="75%"></li>
</ul></li>
<li><p><strong>Silver et al.&nbsp;- 2014:</strong> Released the deterministic policy gradient (DPG) algorithm.</p></li>
<li><p><strong>Lillicrap et al.&nbsp;- 2015:</strong> Improved DPG with deep deterministic policy gradient (DDPG)</p></li>
<li><p><strong>Schulman et al.&nbsp;- 2016:</strong> Released trust region policy optimization (TRPO) and generalized advantage estimation (GAE) methods.</p></li>
<li><p><strong>Sergey Levine et al.&nbsp;- 2016:</strong> Published Guided Policy Search (GPS)</p></li>
<li><p><strong>Silver et al.&nbsp;- 2016:</strong> Demonstrated AlphaGo</p></li>
<li><p>…</p></li>
</ul>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>Morales, M. (2020). Grokking Deep Reinforcement Learning. Originally Published: October 15, 2020.</p>
<p><em>All figures are sourced from this book.</em></p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>Deep Learning</category>
  <category>Reinforcement Learning</category>
  <category>Deep Reinforcement Learning</category>
  <guid>https://ahadjawaid.github.io/personal-website/blog/posts/introduction-to-deep-reinforcement-learning/index.html</guid>
  <pubDate>Tue, 27 Jun 2023 07:00:00 GMT</pubDate>
  <media:content url="https://ahadjawaid.github.io/personal-website/blog/posts/introduction-to-deep-reinforcement-learning/assets/neural.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>From Paper to Code: A Guide to Implement Research</title>
  <dc:creator>Ahad Jawaid</dc:creator>
  <link>https://ahadjawaid.github.io/personal-website/blog/posts/translating-theory-into-code/index.html</link>
  <description><![CDATA[ 



<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Recently, I’ve been diving into the practice of implementing deep learning research papers. This post is designed to guide you through this process, sharing my key insights along the way.</p>
<p>Starting with an independent study course at university, I explored deep learning intensively. To streamline the journey of understanding and implementing such papers, I’ve distilled the process into four steps:</p>
<p><img src="https://ahadjawaid.github.io/personal-website/blog/posts/translating-theory-into-code/assets/read_code_visualize_diagram.svg" class="img-fluid"></p>
<p>Let’s break each one down.</p>
</section>
<section id="read" class="level2">
<h2 class="anchored" data-anchor-id="read">Read</h2>
<p>Reading a research paper should be focused on understanding the process rather than memorizing details. Pay attention to the inputs, outputs, data transformations, and assumptions.</p>
<p><img src="https://ahadjawaid.github.io/personal-website/blog/posts/translating-theory-into-code/assets/questions.svg" class="img-fluid"></p>
<p>To expedite the reading process, it helps to know where to find necessary details. Typically, the most crucial sections in a research paper are:</p>
<ol type="1">
<li><strong>Abstract</strong>
<ul>
<li>Provides a broad understanding of the paper and its relevance.</li>
</ul></li>
<li><strong>Figures</strong>
<ul>
<li>Often encapsulate the paper’s content and are valuable for understanding the model’s workings. However, ensure their accuracy by cross-referencing the textual content.</li>
</ul></li>
<li><strong>Introduction</strong>
<ul>
<li>Highlights the paper’s novelty or significance.</li>
</ul></li>
<li><strong>Architecture (Can go by other names)</strong>
<ul>
<li>Contains detailed descriptions of the architecture (usually where most of your time will be spent).</li>
</ul></li>
<li><strong>Training</strong>
<ul>
<li>Provides hyperparameters for training, model performance, and other essential information.</li>
</ul></li>
<li><strong>The appendix</strong>
<ul>
<li>Often contains implementation details not fitting into the main paper’s flow.</li>
</ul></li>
<li><strong>Experiements</strong> (Optional)
<ul>
<li>Can include important data and parameters used for model training.</li>
</ul></li>
</ol>
</section>
<section id="implement" class="level2">
<h2 class="anchored" data-anchor-id="implement">Implement</h2>
<p>During implementation, the key is not to get overwhelmed by the complexity of the task. Instead, focus on the components you understand, and incrementally build upon them. It’s okay if your code isn’t flawless initially; revisions can be made as you progress.</p>
</section>
<section id="visualize" class="level2">
<h2 class="anchored" data-anchor-id="visualize">Visualize</h2>
<p>Post-implementation, validate your code. Use libraries like <a href="https://matplotlib.org/">matplotlib</a> to visualize the outputs, or tools like <a href="https://docs.pytest.org/en/7.3.x/">pytest</a> to perform tests. Working in an interactive environment like a <a href="https://jupyter.org/">jupyter notebook</a> can also be helpful.</p>
</section>
<section id="repeat" class="level2">
<h2 class="anchored" data-anchor-id="repeat">Repeat</h2>
<p>The key to successful implementation is repetition. Even if you start slowly, your pace will increase as you build your knowledge and develop better abstractions. This process also improves your programming skills.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In summary, the methodology I formulated and the key lessons I learned during my initial experience implementing a research paper can be encapsulated in four words: read, implement, visualize, and repeat. I hope you find this post beneficial, and I welcome any feedback or suggestions you might have. Thank you!</p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>Research</category>
  <guid>https://ahadjawaid.github.io/personal-website/blog/posts/translating-theory-into-code/index.html</guid>
  <pubDate>Mon, 05 Jun 2023 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Neural network: The universal function</title>
  <dc:creator>Ahad Jawaid</dc:creator>
  <link>https://ahadjawaid.github.io/personal-website/blog/posts/the-universal-function/index.html</link>
  <description><![CDATA[ 



<p>Have you ever imagined a single machine that can adapt to any role given to it? It could be a screw, a tire, a screen or any other task you could think of. That’s what a neural network is for software — a function that can perform any task.</p>
<p>Initially, when I learned about neural networks, it seemed like a highly abstract concept that emulated the brain. However, after reading books such as Deep Learning by Ian Goodfellow and The Deep Learning practical coder by Jeremy Howard, I came to realize that a neural network is nothing more than a function that can be adjusted to perform any task we require of it.</p>
<section id="what-exactly-is-a-function" class="level2">
<h2 class="anchored" data-anchor-id="what-exactly-is-a-function">What exactly is a function?</h2>
<p>A function is a mapping of an input to an output. For instance, an addition function could take two numbers as inputs and output their sum.</p>
</section>
<section id="adjustable-function" class="level2">
<h2 class="anchored" data-anchor-id="adjustable-function">Adjustable function</h2>
<p>An adjustable function, on the other hand, is a function that has weights or parameters that regulate how the inputs are modified to generate a specific output. For example, a function f that takes two numbers as inputs can be expressed as</p>
<blockquote class="blockquote">
<p><code>f(input1, input2) = weight1 * input1 + weight2 * input2</code></p>
</blockquote>
<p>Here, we can change the function by adjusting the weights.</p>
</section>
<section id="how-can-an-adjustable-function-learn-a-task" class="level2">
<h2 class="anchored" data-anchor-id="how-can-an-adjustable-function-learn-a-task">How can an adjustable function learn a task?</h2>
<p>Now that we have established what an adjustable function is, let’s look at how we can use it to solve a problem. The process of adjusting the weights in the function to accomplish a task involves three main steps:</p>
<pre><code>1. Get output from the function
2. Check how wrong the output is compared to 3. the desired output
4. Adjust the weights to make the output look more like the desired output</code></pre>
<p>This process can be illustrated in the following diagram:</p>
<p><img src="https://ahadjawaid.github.io/personal-website/blog/posts/the-universal-function/assets/model.png" width="100%"></p>
<p>As simple as this may seem, this is the core of how a neural network learns a task. It starts by guessing an output, and then we adjust the weights to make it look more like the desired output.</p>
</section>
<section id="adjusting-the-weights-automatically" class="level2">
<h2 class="anchored" data-anchor-id="adjusting-the-weights-automatically">Adjusting the weights automatically</h2>
<p>Adjusting the weights automatically is where it gets interesting. Let’s assume you’re playing a game where someone has a number in mind, and you have to guess it. Every time you make a guess, they tell you whether you’re close or far. If you randomly pick the numbers at random, you’d get nowhere. But adding two simple steps to the process can solve this problem:</p>
<pre><code>1. A method to determine how far you are from the desired output (output error method)
2.A method for finding the direction and extent of the weights to be changed (method for determining how to adjust weights)</code></pre>
</section>
<section id="output-error-method" class="level2">
<h2 class="anchored" data-anchor-id="output-error-method">Output Error Method</h2>
<p>To find how far we are from the desired output, we can subtract our function’s output and the desired output,</p>
<blockquote class="blockquote">
<p><code>f(input1, input2) = 5</code> <code>goal_function(input1, input2) = 10</code> <code>error = 5–10 = -5</code></p>
</blockquote>
<p>The problem with this method is that it could give us negative values, which can be problematic when attempting to minimize the error. To address this issue, we can bound the method by taking the absolute value of the subtraction (positive number).</p>
<blockquote class="blockquote">
<p><code>error = | 5–10 | = 5</code></p>
</blockquote>
</section>
<section id="method-for-determining-how-to-adjust-weights" class="level2">
<h2 class="anchored" data-anchor-id="method-for-determining-how-to-adjust-weights">Method for determining how to adjust weights</h2>
<p>The last addition to adjust the weights automatically is to find the direction and extent of the weights to be changed. We can accomplish this by using a concept from calculus known as the derivative, which is the slope of the function at a specific point. By finding the slope or derivative, we can determine the direction and amount to change the weights.</p>
<p><img src="https://ahadjawaid.github.io/personal-website/blog/posts/the-universal-function/assets/tanget.png" width="100%"></p>
<p>So in this illustration, we can find that the slope at point one is two by using the rise-over run of the tangent line (a line that touches the function at a point). We can use this slope or derivative to determine in what direction and how much to change the weights. This is done because of a property that a derivative has, which is when the derivative is zero, it is at the minimum or maximum (or saddle point) of the function. We can take advantage of this property with our method of measuring the error to find the minimum of the error.</p>
<p>So if we keep adjusting the weights and the derivative gets smaller, we are approaching a minimum or maximum (or saddle point). To ensure we are finding the minimum and not the maximum error, we need to figure out what direction we should change the weights. This can be done simply by subtracting the derivative to act as we descend to the minimum. Putting it together</p>
<p>The heart of deep learning lies in the ability of neural networks to learn any task by going through five steps. These are:</p>
<pre><code>1. Input data into the function
2. Compare the output to the error function (loss function)
3. Take the derivative of the error function with respect to the weights
4. Subtract the derivative of the weights from the weights
5. Repeat until the error is small</code></pre>
<p><img src="https://ahadjawaid.github.io/personal-website/blog/posts/the-universal-function/assets/universal-function.png" width="100%"></p>
<p>Everything else is focused on making the training process efficient and timely.</p>
</section>
<section id="the-universal-function" class="level2">
<h2 class="anchored" data-anchor-id="the-universal-function">The Universal Function</h2>
<p>In conclusion, the neural network is a powerful tool that allows software to adapt to any role given to them. At first glance, it may seem like an abstract concept, but it is nothing more guessing and checking then improving. What’s fascinating about this is that the improvement can be made automatically by using concepts from calculus like the derivative, which allows us to determine how to improve. With these tools, the neural network can learn any task and solve most problem thrown its way.</p>
<p>Today, we see the applications of neural networks in various fields, including speech recognition, image and pattern recognition, and natural language processing, to name a few. The possibilities of what we can achieve with this technology are endless, and we are only scratching the surface of what it can do.</p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>Deep Learning</category>
  <category>Neueral Networks</category>
  <guid>https://ahadjawaid.github.io/personal-website/blog/posts/the-universal-function/index.html</guid>
  <pubDate>Thu, 23 Feb 2023 08:00:00 GMT</pubDate>
  <media:content url="https://ahadjawaid.github.io/personal-website/blog/posts/the-universal-function/assets/filling-machine.png" medium="image" type="image/png" height="144" width="144"/>
</item>
</channel>
</rss>
