---
title: "Mathmatical Foundations of Reinforcement Learning"
author: "Ahad Jawaid"
date: "2023-06-28"
image: "assets/blackboard.jpg"
draft: False
description: My notes on Deep Reinforcement Learning (DRL) based on the second chapter of the 'Grokking Deep Reinforcement Learning' by Miguel Morales.
categories: [Deep Learning, Reinforcement Learning, Deep Reinforcement Learning]
---

<style>
.responsive-img {
    width: 100%;
}

@media (min-width: 992px) {
    .responsive-img {
        width: 75%;
    }
}
</style>

## Components of Reinforcement Learning

- **Reinforcement Learning:** tackles the problem of complex sequential decision-making under uncertainty.
    - *Complex:* refers to the vast state and action spaces of the environment the agent must learn.
    - *Sequential:* refers to the delayed consequences of an agent's actions.
    - *Uncertainty:* refers to how we don't know how the agent's actions will effect the environment.

<img src="assets/reinforcement-learning-interaction-cycle.jpg" class="responsive-img" />

### The agent: The decision maker

- **Agents:** is an entity that is soley responsible for makeing decisions

<img src="assets/internal-steps.jpg" class="responsive-img" />

### The environment: Everything else
- The *environment* includes everything external to the agent, beyond the agent's control, and everything that follows the agent's decisions.

<img src="assets/environment-process.jpg" class="responsive-img" />

- **Grid World Environment:** 
- **Bandit Environment:** 

## Components of the Environment:

- The environment is made up of states, actions, transitions probabilities, and a reward function.

### States: Specific configurations of the environment
- A *state* is a complete description of the environment

- **Terminal States:** Is the last state in an eposdic task. All transitions from this state goes back to itself.

- **State Space:** A combiniation of all possible set of variables and value that can describe the environment.

<img src="assets/state-space.jpg" class="responsive-img" />

- **Observation:** Is the set of variables that the agent experiences. It may or maynot be complete as a state must be.

- **Observation Space:** Is all possible values of the variable an agent experiences.

### Actions: A mechanism to influence the environment
- An *action* is a choice an agent can make that may or may not influence the environment.

- **Action Space:** The set of all actions in all states.

### Transition Function: Consequences of agent actions
- A *transition function* outputs the probability of getting to a new state given the current state and action the agent will take. So it dictates what are the chances the agent will transition to the next state.
- **Stochastic Transitions:** Is a transition that is not gaurenteed to go to one different state but may go to different states with a different probability.

<img src="assets/transition-function.jpg" class="responsive-img" />

### Reward Function
- A *reward function* is what guides the agent to achieve it's goal. It maps an agent's state or action to a scalar reward that gives a single of goodness but not correctness of agent's state or  action.

<img src="assets/reward-function.jpg" class="responsive-img" />

- **Return:** 


## Markov Decision Process (MDPs): The engine of the environment

- model virtually any complex sequential decision-making problem under uncer- tainty in a way that RL agents can interact with and learn to solve solely through experience.


### Markov Property

<img src="assets/markov-property.jpg" class="responsive-img" />


### Horizon: Time changes whatâ€™s optimal


### Discount: The future is uncertain, value it less

<img src="assets/effect-of-discount-factor.jpg" class="responsive-img" />

<img src="assets/discount-factor.jpg" class="responsive-img" />



### Extensions to MDPs



### Putting it all together

<img src="assets/mdps-vs-pomdps.jpg" class="responsive-img" />


## References

Morales, M. (2020). Grokking Deep Reinforcement Learning. Originally Published: October 15, 2020.

_All figures are sourced from this book._